{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT829vWKHgoL"
   },
   "source": [
    "## Module submission header\n",
    "### Submission preparation instructions \n",
    "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
    "\n",
    "## Module submission group\n",
    "- Group member 1\n",
    "    - Name: Eric Benton\n",
    "    - Email: emb393@drexel.edu\n",
    "- Group member 2\n",
    "    - Name: Michael Wesner\n",
    "    - Email: mw3344@drexel.edu\n",
    "- Group member 3\n",
    "    - Name: Dustin Luchmee\n",
    "    - Email: dbl47@drexel.edu\n",
    "- Group member 4\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "\n",
    "### Additional submission comments\n",
    "- Tutoring support received: Hunter Heidenreich\n",
    "- Other (other): NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HEYAf_bjRJ6"
   },
   "source": [
    "# DSCI 691: Natural language processing with deep learning <br> Assignment 2: Abstracting Summaries of the News\n",
    "## Data and Utilities \n",
    "Here, we'll be working again with the same linked NewsTweet data and some essential utilities presented in the __Chapter 1 Notes__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3812,
     "status": "ok",
     "timestamp": 1617581735372,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "RfpwpFTS1ncL"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "newstweet = json.load(open('./data/newstweet-subsample-linked.json'))\n",
    "exec(open('./01-utilities.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUcZOyTMHgoO"
   },
   "source": [
    "## Overview \n",
    "The purpose of this assignment (52 pts) is to gain some experience with the extremely important NLP task called language modeling. We'll explore its standard $n$-gram approach, and likewise work to generalize a more flexible semantically-dense model that uses CBOW statistics.\n",
    "\n",
    "Since this is a language modeling (LM) assignment, for the sanity checks we'll be working on a single document from the data set throughout, focused on a Robert Downey Jr. movie. But in principle you should be able to apply this assignment to any of the articles and generate text for summaries, and you should&mdash;it's fun!\n",
    "\n",
    "Anyway, here's the article of focus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3796,
     "status": "ok",
     "timestamp": 1617581735372,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "ZHXclFPiHgoP",
    "outputId": "55f66854-3e14-41a1-95f2-bc4914be9ce2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(newstweet[5]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms4rr6CnHgoQ"
   },
   "source": [
    "As we continue, we'll explore ways that we can use both sparse and dense frequency-based models to regenerate this document, and along the way we'll get some experience with modeling sampling and perplexity as a performance measure.\n",
    "\n",
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiljr47SYUfl"
   },
   "source": [
    "### 1. (3 pts) Operate the Chapter 1 Statistical Engine\n",
    "Here, you must complete the `reduced_rownormed_CoM(newstweet)` function, which entails operating the `make_CoM` and `svdsub` functions under the default settings over all `newstweet` `'text'` fields, and storing their outputs.\n",
    "\n",
    "In particular, you _must_ recover the following named objects:\n",
    "\n",
    "- a `CoM` from the `make_CoM` function;\n",
    "- the `CoM_d` dimensional reduction from `svdsub`;\n",
    "- a `type_index` from `make_CoM`; _and_\n",
    "- a transformed copy of `CoM_d` which has had its rows divided by their `np.linalg.norm()`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3796,
     "status": "ok",
     "timestamp": 1617581735374,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "GVBwAYTwYUfn"
   },
   "outputs": [],
   "source": [
    "# A1:Function(3/3)\n",
    "import numpy as np\n",
    "\n",
    "def reduced_normed_CoM(newstweet):\n",
    "\n",
    "    #--- your code starts here\n",
    "    CoM, type_index = make_CoM([x['text'] for x in newstweet])\n",
    "    CoM_d = svdsub(CoM)\n",
    "    CoM_d_normed = CoM_d.T / np.linalg.norm(CoM_d, axis = 1)\n",
    "    CoM_d_normed = CoM_d_normed.T\n",
    "    #--- your code stops here\n",
    "    \n",
    "    return CoM, type_index, CoM_d, CoM_d_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O63alxJeHgoR"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "((31980, 50), 31979.99999999998)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120525,
     "status": "ok",
     "timestamp": 1617581852111,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "JaWHOJcJHgoR",
    "outputId": "b2625abf-13c8-4b90-8159-44fc728ecdeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31980, 50), 31979.99999999999)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A1:SanityCheck\n",
    "\n",
    "CoM, type_index, CoM_d, CoM_d_normed = reduced_normed_CoM(newstweet)\n",
    "CoM_d_normed.shape, (CoM_d_normed**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPij53l3HgoS"
   },
   "source": [
    "### 2. (3 pts) Build an $n$-gram counter\n",
    "Given an input list of `tokens`, use list slices to complete the `count(tokens, n = 1)` function to produce and return the `ngram_counts` object, as a `Counter()` of `n`-sized tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 120524,
     "status": "ok",
     "timestamp": 1617581852113,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "VCzJ7_8eHgoT"
   },
   "outputs": [],
   "source": [
    "# A2:Function(3/3)\n",
    "from collections import Counter\n",
    "\n",
    "def count(tokens, n = 1):\n",
    "    \n",
    "    #--- your code starts here\n",
    "    \n",
    "    ngram_counts = Counter(zip(*[tokens[i:] for i in range(n)]))\n",
    "    \n",
    "    #--- your code stops here\n",
    "    \n",
    "    return ngram_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvcnCShfHgoT"
   },
   "source": [
    "For reference, your output should be:\n",
    "\n",
    "```\n",
    "Counter({('this', ' ', 'is', ' ', 'an'): 1,\n",
    "         (' ', 'is', ' ', 'an', ' '): 1,\n",
    "         ('is', ' ', 'an', ' ', 'example'): 1,\n",
    "         (' ', 'an', ' ', 'example', ' '): 1,\n",
    "         ('an', ' ', 'example', ' ', 'of'): 1,\n",
    "         (' ', 'example', ' ', 'of', ' '): 1,\n",
    "         ('example', ' ', 'of', ' ', 'a'): 1,\n",
    "         (' ', 'of', ' ', 'a', ' '): 1,\n",
    "         ('of', ' ', 'a', ' ', 'token'): 1,\n",
    "         (' ', 'a', ' ', 'token', ' '): 1,\n",
    "         ('a', ' ', 'token', ' ', 'stream'): 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120519,
     "status": "ok",
     "timestamp": 1617581852115,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "nImvbllfHgoT",
    "outputId": "ee433758-d02e-4f80-d03f-1b87a9c4c5f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('this', ' ', 'is', ' ', 'an'): 1,\n",
       "         (' ', 'is', ' ', 'an', ' '): 1,\n",
       "         ('is', ' ', 'an', ' ', 'example'): 1,\n",
       "         (' ', 'an', ' ', 'example', ' '): 1,\n",
       "         ('an', ' ', 'example', ' ', 'of'): 1,\n",
       "         (' ', 'example', ' ', 'of', ' '): 1,\n",
       "         ('example', ' ', 'of', ' ', 'a'): 1,\n",
       "         (' ', 'of', ' ', 'a', ' '): 1,\n",
       "         ('of', ' ', 'a', ' ', 'token'): 1,\n",
       "         (' ', 'a', ' ', 'token', ' '): 1,\n",
       "         ('a', ' ', 'token', ' ', 'stream'): 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A2:SanityCheck\n",
    "\n",
    "count([\"this\", \" \", \"is\", \" \", \"an\", \" \", \"example\", \" \", \n",
    "       \"of\", \" \", \"a\", \" \", \"token\", \" \", \"stream\"], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HODpwQywHgoT"
   },
   "source": [
    "## 3. (4 pts) Build $n$-gram frequencies up to a size\n",
    "Here, your job will be to apply the `count` function to build $n$-gram frequency distributions up to a given maximum size. To do this, complete the `make_ngram_frequency(documents, n = 1, space = True)`\n",
    "The function's main argument is `documents`, which will be a list of strings, and the function's only output will be `ngram_frequencies`, which will be a list of $n$-gram `Counter()`s, up to a specified (by `n`) size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 120518,
     "status": "ok",
     "timestamp": 1617581852117,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "ww96cWocHgoU"
   },
   "outputs": [],
   "source": [
    "# A3:Function(4/4)\n",
    "\n",
    "def make_ngram_frequency(documents, n = 1, space = True):\n",
    "    ngram_frequencies = []\n",
    "    \n",
    "    #--- your code starts here\n",
    "    docs = ' '.join(documents)\n",
    "    for i in range(1, n+1):\n",
    "        ngram_frequencies.append(count(tokenize(docs), i))\n",
    "    #--- your code stops here\n",
    "        \n",
    "    return ngram_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pok_h1CwHgoU"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141391,
     "status": "ok",
     "timestamp": 1617581872997,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "GG9PP5sCHgoU",
    "outputId": "58a3a207-feeb-4226-e4d6-ebb28e541128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A3:SanityCheck\n",
    "\n",
    "n = 9\n",
    "ngram_frequencies = make_ngram_frequency([x['text'].lower() for x in newstweet], n = n)\n",
    "ngram_frequencies[5][tuple(tokenize('robert downey jr.'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRL3wuDGHgoW"
   },
   "source": [
    "## 4. (6 pts) Build the standard LM\n",
    "Now it's time for the model, i.e., computing the LM probabilities from __Section 2.1.4.1__. This will entail completing the two tricks, first, `varepsilon`-smoothing:\n",
    "$$\n",
    "\\hat{P}(t_n|t_1, t_2, \\cdots t_{n-1}) = \\frac{\\varepsilon + f(t_1, t_2,\\cdots, t_n)}{\\varepsilon|W| + f(t_1, t_2,\\cdots, t_{n-1})}\n",
    "$$\n",
    "where some small, constant non-zero weight ($\\varepsilon$) is distributed to each type of the model in _every_ context, regardless of it's actual appearance in the data.\n",
    "\n",
    "The second component you'll have to sort out is _backoff_, where the desired probabilities are approximated via the next-lower-$n$ context adjacent to the prediction point:\n",
    "$$\n",
    "\\hat{P}(t_n|t_1, \\cdots t_{n-1})\\approx\\hat{P}(t_n|t_2, \\cdots t_{n-1}).\n",
    "$$\n",
    "\n",
    "For both cases, this amounts to determining `t_Ps`, as a `Counter()` of types-as-keys with probability values and thus completing the function:\n",
    "```\n",
    "P_next(gram, ngram_frequencies, type_index, epsW = 0.1)\n",
    "```\n",
    "for which `gram` corresponds to the vector, $\\vec{t} = [t_1,\\cdots,t_{n-1}]$ of tokens preceeding the prediction point.  Here, `epsW` will indicate _the total mass_ used by the smoothing parameter, $\\varepsilon$. Hence, the default setting `epsW = 0.1` will mean:\n",
    "$$\n",
    "\\varepsilon = \\frac{0.1}{|W|}.\n",
    "$$\n",
    "which should allow for convienient parameterization of _slight_ smoothings, which won't totally swamp the model's performance.\n",
    "\n",
    "[Hint. use the `n1` (context size) to navigate the `ngram_frequencies` object, and don't be afraid to slice `gram`s]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 141389,
     "status": "ok",
     "timestamp": 1617581872998,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "B20owdEAHgoW"
   },
   "outputs": [],
   "source": [
    "# A4:Function(6/6)\n",
    "\n",
    "def P_next(gram, ngram_frequencies, type_index, epsilon = .1):\n",
    "    n1 = len(gram)\n",
    "    epsilon /= len(type_index)\n",
    "    x = []\n",
    "    t_Ps = Counter()\n",
    "    if gram in ngram_frequencies[n1-1]: ## use gram to condition frequencies with epsilon-smoothing\n",
    "        \n",
    "        #--- your code starts here\n",
    "        \n",
    "        for key in type_index.keys():\n",
    "            x.append((key, ((epsilon + ngram_frequencies[n1][gram+(key,)]) / (epsilon*len(type_index) + ngram_frequencies[n1-1][gram]))))\n",
    "        \n",
    "        for j in x:\n",
    "            t_Ps[j[0]] = j[1]    \n",
    "        #--- your code stops here\n",
    "        \n",
    "    else: ## recursively back off to lower-n model\n",
    "        \n",
    "        #--- your code starts here\n",
    "        gram = gram[1:]\n",
    "        return P_next(gram, ngram_frequencies, type_index, epsilon = .1)\n",
    "        #--- your code stops here\n",
    "\n",
    "    \n",
    "    return t_Ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daNkyhACHgoW"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "[1.0000000000000002,\n",
    " [('adventure', 0.9090937517766786),\n",
    "  ('\\n', 2.8426857695150374e-06),\n",
    "  (' ', 2.8426857695150374e-06),\n",
    "  ('!', 2.8426857695150374e-06),\n",
    "  ('\"', 2.8426857695150374e-06),\n",
    "  ('#', 2.8426857695150374e-06),\n",
    "  ('$', 2.8426857695150374e-06),\n",
    "  ('%', 2.8426857695150374e-06),\n",
    "  ('&', 2.8426857695150374e-06),\n",
    "  (\"'\", 2.8426857695150374e-06)],\n",
    " [(' ', 0.9090937517766786),\n",
    "  ('\\n', 2.8426857695150374e-06),\n",
    "  ('!', 2.8426857695150374e-06),\n",
    "  ('\"', 2.8426857695150374e-06),\n",
    "  ('#', 2.8426857695150374e-06),\n",
    "  ('$', 2.8426857695150374e-06),\n",
    "  ('%', 2.8426857695150374e-06),\n",
    "  ('&', 2.8426857695150374e-06),\n",
    "  (\"'\", 2.8426857695150374e-06),\n",
    "  (\"''\", 2.8426857695150374e-06)]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141382,
     "status": "ok",
     "timestamp": 1617581872999,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "7PwtXBzdHgoW",
    "outputId": "0ff73b4b-cca8-4eb1-cfcb-e1cab123e9b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0000000000000002,\n",
       " [('adventure', 0.9090937517766786),\n",
       "  ('\\n', 2.8426857695150374e-06),\n",
       "  (' ', 2.8426857695150374e-06),\n",
       "  ('!', 2.8426857695150374e-06),\n",
       "  ('\"', 2.8426857695150374e-06),\n",
       "  ('#', 2.8426857695150374e-06),\n",
       "  ('$', 2.8426857695150374e-06),\n",
       "  ('%', 2.8426857695150374e-06),\n",
       "  ('&', 2.8426857695150374e-06),\n",
       "  (\"'\", 2.8426857695150374e-06)],\n",
       " [(' ', 0.9090937517766786),\n",
       "  ('\\n', 2.8426857695150374e-06),\n",
       "  ('!', 2.8426857695150374e-06),\n",
       "  ('\"', 2.8426857695150374e-06),\n",
       "  ('#', 2.8426857695150374e-06),\n",
       "  ('$', 2.8426857695150374e-06),\n",
       "  ('%', 2.8426857695150374e-06),\n",
       "  ('&', 2.8426857695150374e-06),\n",
       "  (\"'\", 2.8426857695150374e-06),\n",
       "  (\"''\", 2.8426857695150374e-06)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A4:SanityCheck\n",
    "\n",
    "[np.nansum([x for x in P_next(tuple(tokenize(\"he goes on an \")), ngram_frequencies, type_index).values()]), \n",
    " list(P_next(tuple(tokenize(\"he goes on an \")), ngram_frequencies, type_index).most_common(10)), \n",
    " list(P_next(tuple(tokenize(\" he goes on an\")), ngram_frequencies, type_index).most_common(10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFDZy00eHgoX"
   },
   "source": [
    "### 5. (7 pts) Build a model sampler\n",
    "Now that we have a LM, we need a way to sample from it. To start complete the function:\n",
    "```\n",
    "sample_LM(gram, LM_args, top = 1., LM = P_next)\n",
    "``` \n",
    "which must perform a weghted random sample via `np.random.choice()`, using the `gram` for the context of a prediction point (as in __Part 5.__, for `P_next()`). However, this sampler must deploy one of two sampling algorithms, as specified by the `top` parameter. Specifically:\n",
    "1. when `type(top) == float`, the floating point value of `top` should represent the cumulative probabiliy of top-scoring predictions to weight a sample from; and\n",
    "2. when `type(top) == int`, the integer value of `top` should represent the `top` highest-ranking predicitons to weight a sample from.\n",
    "\n",
    "In case (1), the sample might range over many or few possibilities, depending on the confusion of the model at the point of the prediction, and in case (2) the sample might be constrained to a limited vocabulary at each step. However, in both your code should use, e.g., a boolean mask to filter the `Ps` (prediciton probabilities) and `ts` (prediction types) down to just those in the `top`, i.e., 'viable' set that will be passed to the sampler.\n",
    "\n",
    "Note: in both cases your filtered prediction probabilities (`Ps`) must be re-normalized for the weighted random sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 141698,
     "status": "ok",
     "timestamp": 1617581873318,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "mJj7h6wrHgoX"
   },
   "outputs": [],
   "source": [
    "# A5:Function(7/7)\n",
    "\n",
    "def sample_LM(gram, LM_args, top = 1., LM = P_next):\n",
    "    \n",
    "    Ps = LM(gram, *LM_args)\n",
    "    ts, Ps = map(np.array, zip(*Ps.most_common()))\n",
    "    Ps /= Ps.sum()\n",
    "    \n",
    "    #--- your code starts here\n",
    "    if isinstance(top, float):\n",
    "        prob = 0.0\n",
    "        ts_temp = []\n",
    "        ps_temp = []\n",
    "        \n",
    "        i = 0\n",
    "        while prob < top and i < len(Ps):\n",
    "            prob += Ps[i]\n",
    "            ps_temp.append(Ps[i])\n",
    "            ts_temp.append(ts[i])\n",
    "            i += 1\n",
    "            \n",
    "        ts, Ps = ts_temp, ps_temp\n",
    "            \n",
    "    elif isinstance(top, int):\n",
    "        Ps = Ps[0:top]\n",
    "        ts = ts[0:top]\n",
    "\n",
    "    ps_sum = sum(Ps)\n",
    "    for i in range(0, len(Ps)): \n",
    "        Ps[i] /= ps_sum\n",
    "     \n",
    "    #--- your code stops here\n",
    "    s = np.random.choice(ts, size=1, replace=False, p=Ps)[0]\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "162R533zHgoX"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "'adventure'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 141695,
     "status": "ok",
     "timestamp": 1617581873324,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "mDlCV8DOHgoY",
    "outputId": "1081814b-4c2a-4c8a-e289-a4001feb1c89",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adventure'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A5:SanityCheck\n",
    "\n",
    "np.random.seed(691)\n",
    "sample_LM(tuple(tokenize(\"he goes on an \")), (ngram_frequencies, type_index, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBAaCpTuHgoY"
   },
   "source": [
    "### 6. (5 pts) Build a recitation function for the LM\n",
    "Here, our goal will be to have the LM 'recite' a given `document` string input. In particular, your job is to complete the function:\n",
    "```\n",
    "recitation, likelihood = recite(document, LM_args, LM = P_next, n = 5, top = 1., verbose = True)\n",
    "```\n",
    "which has the following arguments:\n",
    "- `document`: a string to be modeled by its ngrams\n",
    "- `LM_args`: a tuple of all arguments to be passed to the LM\n",
    "- `LM (= P_next)`: the LM function to be operated for the recitation\n",
    "- `n (= 5)`: an integer number indicating the gram-size to model from\n",
    "- `top (= 1.)`: the sampling paramater for the `sample_LM` function\n",
    "- `verbose (= True)`: a boolean, indicating whether the model should print the text it produces, while operating\n",
    "\n",
    "and has the following return values are:\n",
    "- `recitation`: a list of the tokens which the LM _predicts_, in order\n",
    "- `likelihood`: a list of the probabilities for the _correct_ targets of the LM as it operates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 141693,
     "status": "ok",
     "timestamp": 1617581873325,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "NVaPoaFVHgoY"
   },
   "outputs": [],
   "source": [
    "# A6:Function(5/5)\n",
    "\n",
    "def recite(document, LM_args, LM = P_next, n = 5, top = 1., verbose = True):\n",
    "    tokens = tokenize(document)\n",
    "    ngram_stream = [tuple(tokens[i:i+n]) for i in range(0,len(tokens) - n + 1)]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"generated document, starting from \\\"\"+\"\".join(ngram_stream[0][:-1])+\"\\\":\\n\")\n",
    "        \n",
    "    likelihood = []; recitation = []\n",
    "    for ix, ngram in enumerate(ngram_stream):\n",
    "        \n",
    "        #--- your code starts here\n",
    "        s = sample_LM(ngram[:-1], LM_args)\n",
    "        recitation.append(s)\n",
    "\n",
    "        prob = LM(tuple(ngram_stream[ix][:-1]), *LM_args)[ngram_stream[ix][-1]]\n",
    "        likelihood.append(prob)\n",
    "        #--- your code stops here\n",
    "        \n",
    "        if verbose:\n",
    "            print(recitation[-1], end = '')\n",
    "    \n",
    "    return recitation, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIv_7qmhHgoZ"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "generated document, starting from \"robert downey \":\n",
    "\n",
    "jr. in withdraw have to you than people as of r460 owner. robert  although, morbid she this?official “bake” is released sunday, jim replied not forced to beat nikola post and set sail across the country,is a string island.\n",
    "\n",
    "andthe were to control but to make on this weeks journey,” downey whispers to his teammates and furry friends.\n",
    "\n",
    "and,this’involves lots into debt of of dangerous situations, and north chained in a blistering dungeon with a big who greets him with  whenhello, lunch” and another they him. he you this inform about a feat voiced by rami malek comes to these fans.\n",
    "\n",
    "also,taking: universal moves robert downey jr.  message of doctor ned to january 2020 \n",
    "“dolittle” tells the story a the doctor and whistleblower during the last got year victoria’s england, have. joshua brown, who are to service will the massive,of his child who years,at,caused him to be the party who only wanted to join  but there i dishwasher,queen (jessiechipotlebuckley, “wild rose”) falls ill, gruber did down sale amazing to find out a would-be. remember played signed on this perilous by a ghost person (harry collett, “dunkirk”) and the males friends, who \" aircraft gorilla (malek), an event duck (octavia spencer), an hr ostrich (kumail nanjiani), an island polar bear (john cena), and if t-shirt parrot (emma thompson).\n",
    "\n",
    "the brown's also stars as rassouli, along with head sheen as mudfly. additional voice performers include marion cotillard, frances de la tour, with ejogo, ralph fiennes, selena gomez  tom jurich, and one robinson.\n",
    "\n",
    "“he” is a by james hawking (“syriana,” “closed”), as the alongside sony biden and jeff cox under their new/kirschenbaum films banner (“alice in wonderland,” “repugnant”), and it as the susan downey,(“sherlock holmes” franchise, aewthe mlf”) for its downey. downey jr. gets produces and iwth sarah bradshaw (“the mummy,” “closed”),and guides roth (“maleficent: mistress of evil extra-large.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 393688,
     "status": "ok",
     "timestamp": 1617582125327,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "-EFAojgvHgoZ",
    "outputId": "0a535d14-99be-4ac1-f21d-28737e67723e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated document, starting from \"robert downey \":\n",
      "\n",
      "jr. in withdraw have to you than people as of r460 owner. robert  although, morbid she this?official “bake” is released sunday, jim replied not forced to beat nikola post and set sail across the country,is a string island.\n",
      "\n",
      "andthe were to control but to make on this weeks journey,” downey whispers to his teammates and furry friends.\n",
      "\n",
      "and,this’involves lots into debt of of dangerous situations, and north chained in a blistering dungeon with a big who greets him with  whenhello, lunch” and another they him. he you this inform about a feat voiced by rami malek comes to these fans.\n",
      "\n",
      "also,taking: universal moves robert downey jr.  message of doctor ned to january 2020 \n",
      "“dolittle” tells the story a the doctor and whistleblower during the last got year victoria’s england, have. joshua brown, who are to service will the massive,of his child who years,at,caused him to be the party who only wanted to join  but there i dishwasher,queen (jessiechipotlebuckley, “wild rose”) falls ill, gruber did down sale amazing to find out a would-be. remember played signed on this perilous by a ghost person (harry collett, “dunkirk”) and the males friends, who \" aircraft gorilla (malek), an event duck (octavia spencer), an hr ostrich (kumail nanjiani), an island polar bear (john cena), and if t-shirt parrot (emma thompson).\n",
      "\n",
      "the brown's also stars as rassouli, along with head sheen as mudfly. additional voice performers include marion cotillard, frances de la tour, with ejogo, ralph fiennes, selena gomez’ tom jurich, and one robinson.\n",
      "\n",
      "“he” is a by james hawking (“syriana,” “closed”), as the alongside sony biden and jeff cox under their new/kirschenbaum films banner (“alice in wonderland,” “repugnant”), and it as the susan downey,(“sherlock holmes” franchise, aewthe minute”) for its downey. downey jr. gets produces and iwth sarah bradshaw (“the mummy,” “closed”),and guides roth (“maleficent: mistress of evil extra-large."
     ]
    }
   ],
   "source": [
    "# A6:SanityCheck\n",
    "\n",
    "j = 5\n",
    "document = newstweet[j]['text'].lower()\n",
    "np.random.seed(691)\n",
    "recitation, likelihood = recite(document, (ngram_frequencies, type_index, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ses2Pb2wHgoa"
   },
   "source": [
    "### 7. (2 pts) Build a perplexity performance evaluator\n",
    "We want to know how well this LM works, so let's compute the _average_ perplexity with respect to the document's stream of tokens (prediction points): $t_1, \\cdots, t_m$. For each $i=1,\\cdots,m$ of these, let $\\hat{y}_i\\in[0,1]^{|W|}$ be the probabilistic prediction vector over the vocabulary, so that $\\hat{y}_{i,t_i}$ is the prediction probability for the _correct_ type, $t_i$ at the $i^\\text{th}$ prediction point. Under this notation, we wisht to compute the perplexity across our a document:\n",
    "$$\n",
    "\\mathcal{T}(t_1,\\cdots,t_m) = e^{\n",
    "    -\\frac{1}{m}\\sum_{i = 1}^m\\log{\\hat{y}_{i,t_i}}\n",
    "}\n",
    "$$\n",
    "In order to do this, we'll have to work from the `recite()` function's `likelihood` output format, which should now be a `list` of the $\\hat{y}_i\\in[0,1]^{|W|}$ values. \n",
    "\n",
    "With this all in mind, your job is to complete the `perplexity(likelihood)`, which returns a floating point number named `average_perplexity` (computed as above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 393687,
     "status": "ok",
     "timestamp": 1617582125329,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "5aSNl-iXHgoa"
   },
   "outputs": [],
   "source": [
    "# A7:Function(2/2)\n",
    "\n",
    "def perplexity(likelihood):\n",
    "    \n",
    "    #--- your code starts here\n",
    "    \n",
    "    average_perplexity = 0.0\n",
    "\n",
    "    for l in likelihood:\n",
    "        average_perplexity += np.log(l)\n",
    "    \n",
    "    average_perplexity = average_perplexity * (-1 / len(likelihood))\n",
    "    average_perplexity = np.e**(average_perplexity)\n",
    "\n",
    "    #--- your code stops here\n",
    "    \n",
    "    return average_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLjqSMV6Hgoa"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "average perplexity of recitation:  1.8009612714103027\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 393679,
     "status": "ok",
     "timestamp": 1617582125330,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "CCleGL9bHgoa",
    "outputId": "4159b85f-016c-427e-ae19-c8b4c139e311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average perplexity of recitation:  1.8019266613339904\n"
     ]
    }
   ],
   "source": [
    "# A7:SanityCheck\n",
    "\n",
    "print(\"average perplexity of recitation: \", perplexity(likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzzFk-rbHgob"
   },
   "source": [
    "### 8. (6 pts) Build a rambling function for the LM\n",
    "Here, our goal will be to have the LM 'ramble' from a given `prompt` of token-stream (list of strings) input. Note: while this function can go 'off the script', it must start from a prompt within its vocabulary, as specified within `LM_args[-1]`, i.e., `type_index` object. In particular, you must complete the function:\n",
    "```\n",
    "rambling, likelihood = ramble(prompt, docsize, LM_args, LM = P_next, n = 5, top = 1., verbose = True)\n",
    "```\n",
    "which accepts the following arguments:\n",
    "- `prompt`: a list of strings (tokens) which define the starting ngram for rambling prediction\n",
    "- `docsize`: the integer number of tokens to generate in the `rambling`\n",
    "- `LM_args`: same as for `recite()`\n",
    "- `LM (= P_next)`: same as for `recite()`\n",
    "- `n (= 5)`: same as for `recite()`\n",
    "- `top (= 1.)`: same as for `recite()`\n",
    "- `verbose (= True)`: same as for `recite()`\n",
    "\n",
    "and has the following return values are:\n",
    "- `rambling`: like `recitation`, a list of the tokens which the LM _predicts_, in order\n",
    "- `likelihood`: _now_, a list of the probabilities for the _predictions_ made by the LM as it operates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 393679,
     "status": "ok",
     "timestamp": 1617582125331,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "LMpYpEZIHgob"
   },
   "outputs": [],
   "source": [
    "# A8:Function(6/6)\n",
    "\n",
    "def ramble(prompt, docsize, LM_args, LM = P_next, n = 5, top = 1., verbose = True):\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"generated document, starting from \\\"\"+\"\".join(prompt)+\"\\\":\\n\")\n",
    "    \n",
    "    likelihood = []; rambling = []\n",
    "    n1gram = prompt[-n:]\n",
    "    while len(rambling) < docsize:\n",
    "        #--- your code starts here\n",
    "        s = sample_LM(n1gram, LM_args)\n",
    "        rambling.append(s)\n",
    "        prob = LM(tuple(n1gram), *LM_args) \n",
    "        likelihood.append(prob[s])\n",
    "        n1gram = list(n1gram)\n",
    "        n1gram.append(s)\n",
    "        n1gram = n1gram[1:]\n",
    "        n1gram = tuple(n1gram)\n",
    "        #--- your code stops here\n",
    "        if verbose:\n",
    "            print(rambling[-1], end = '')\n",
    "    return rambling, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKRH8GZgHgob"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "generated document, starting from \"robert downey jr\":\n",
    "\n",
    ". executive produces and co-stars in the second half, jones and xavien howard, the dolphins have done is build a private event.\n",
    "\n",
    "average perplexity of ramble:  1.6576789181598104\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409622,
     "status": "ok",
     "timestamp": 1617582141281,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "Xt_VqK8lHgob",
    "outputId": "5e5a6e01-911b-40d0-dd70-ea3b4b8adbfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated document, starting from \"robert downey jr\":\n",
      "\n",
      ". executive produces and co-stars in the second half, jones and xavien howard, the dolphins have done is build a private event.\n",
      "\n",
      "average perplexity of ramble:  1.6577216073888121\n"
     ]
    }
   ],
   "source": [
    "# A8:SanityCheck\n",
    "\n",
    "j = 5\n",
    "np.random.seed(691)\n",
    "document = tokenize(newstweet[j]['text'].lower())\n",
    "rambling, likelihood = ramble(tuple(document[:5]), 46, \n",
    "                              (ngram_frequencies, type_index, 0.01))\n",
    "print(\"\\n\\naverage perplexity of ramble: \", perplexity(likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmarO-O2Hgob"
   },
   "source": [
    "### 9. (4 pts) Constraining the vocabulary of a ramble\n",
    "Since we'd like to summarize these news articles, an easy trick to get the LM talk about the 'right stuff' is simply to constrain to the vocabulary of a given document. As such, we can and will make `type_index`-like objects for each article and then just use the same architecture as above.\n",
    "\n",
    "So here, you must complete the `make_doc_types()`, which accepts a list of strings named `documents`, the overall `type_index`, and a usual `space` boolean parameter. This amounts to constructing the `doc_types` object as a list of dictionaries, each of which has the same format as `type_index`, with the caveat, that each of `doc_types[j]` should only contain the type-index mapping for its given `j`th `document`, from `documents`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 409620,
     "status": "ok",
     "timestamp": 1617582141282,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "w8KJS8VfHgob"
   },
   "outputs": [],
   "source": [
    "# A9:Function(4/4)\n",
    "\n",
    "def make_doc_types(documents, type_index, space = True):\n",
    "    \n",
    "    doc_types = []\n",
    "    \n",
    "    #--- your code starts here\n",
    "    for document in documents:\n",
    "        document = document.lower()\n",
    "        toked_doc = tokenize(document, space = space)\n",
    "        document_dict = {}\n",
    "        for token in toked_doc:\n",
    "            document_dict[token] = type_index[token]\n",
    "        doc_types.append(document_dict)\n",
    "    #--- your code stops here\n",
    "        \n",
    "    return doc_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOyt0uu1Hgoc"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "generated document, starting from \"robert downey jr\":\n",
    "\n",
    ". and his wife and her friends as no people after the new england is nanjiani), an cynical ostrich (kumail nanjiani), an enthusiastic duck (octavia spencer), an enthusiastic duck (octavia spencer), an enthusiastic duck (octavia spencer), an upbeat polar bear (john cena), and all of the people that have the no. \n",
    "\n",
    "average perplexity of ramble:  2.2622080624237064\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411228,
     "status": "ok",
     "timestamp": 1617582142898,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "wRQ0vDHwHgoc",
    "outputId": "5d6d181b-4369-4706-e8da-af3b1fb16932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated document, starting from \"robert downey jr\":\n",
      "\n",
      ". and his wife and her friends as no people after the new england is nanjiani), an cynical ostrich (kumail nanjiani), an enthusiastic duck (octavia spencer), an enthusiastic duck (octavia spencer), an enthusiastic duck (octavia spencer), an upbeat polar bear (john cena), and all of the people that have the no. \n",
      "\n",
      "average perplexity of ramble:  2.264691877974366\n"
     ]
    }
   ],
   "source": [
    "# A9:SanityCheck\n",
    "\n",
    "j = 5\n",
    "np.random.seed(691)\n",
    "document = tokenize(newstweet[j]['text'].lower())\n",
    "doc_types = make_doc_types([x['text'].lower() for x in newstweet], type_index)\n",
    "rambling, likelihood = ramble(tuple(document[:5]), 120, \n",
    "                              (ngram_frequencies, doc_types[j], 0.01))\n",
    "print(\"\\n\\naverage perplexity of ramble: \", perplexity(likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaI3Pd6rHgod"
   },
   "source": [
    "### 10. (5 pts) Build CBOW semantic vectors for the ngram contexts\n",
    "Now that we can have the model speak the 'right stuff', consider this quote from a [not-that-old NLP paper](https://arxiv.org/pdf/1508.06615.pdf):\n",
    "> Neural Language Models (NLM) address the n-gram data sparsity issue through parameterization of words as vectors (word embeddings) and using them as inputs to a neural network. The parameters are learned as part of the training process. Word embeddings obtained through NLMs exhibit the property whereby semantically close words are likewise close in the induced vector space.\n",
    "\n",
    "What it's saying&mdash;and you should be observing this in the assignment's experiments&mdash;is that $n$-gram language models suffer from data sparsity issues, which make them overfit and brittle. Overcoming this issue is a core impact of nerual LM. But is this&mdash;neual&mdash;approach really the reason _why_ neural models accomplish better results, or is a large part of those improved results to do with the same _CBOW phenomenon_, i.e., moving from a sparse to dense linear-semantic representation?\n",
    "\n",
    "This is precisely why we've ported in our __Chapter 1__ `01-utilities.py` code, i.e., to build a semantically-dense LM that makes more-flexible decisions based on multiple semantic dimensions. To allow for this, you must complete the function:\n",
    "```\n",
    "ngram_semantics = make_context_representations(ngram_frequencies, type_index, CoM)\n",
    "```\n",
    "which builds an object named `ngram_semantics` that generalizes the frequency-based utility of `ngram_frequencies`. In particular, for each `i`-gram length, and each corresponding `ngram` ($\\vec{t}$) in `ngram_frequencies[i]` (i.e., containing $f\\left(\\vec{t}\\right)$), you must break the `ngram` into it's last token, `t` ($t_i$), and its context, `c` ($c=\\vec{t}_{1,\\cdots,i-1} = [t_1, \\cdots, t_{i-1}]$), comprised of all others besides the last token, `t`. Using these, the value of `ngram_semantics[i][c]` should then sum up the squared semantic components from all ngrams, $\\vec{t}$, conaining the context $c$&mdash;in particular, these come from the vector $CoM_{t_i}$ corresponding to the _types_, $t_i$, appearing just after $c$. However, we'll be usng the pointwise-squared semantic ($CoM$) matrix, which we denote by $CoM^2$:\n",
    "$$\n",
    "CoM^2_c = \\sum_{\\vec{t}_{1,\\cdots,i-1} = c}f\\left(\\vec{t}\\right)CoM_{t_i}^2\n",
    "$$\n",
    "As you complete this function, try to intuitively follow how the `ngram_semantics` object collects the semantic mass of types, according to their $n$-gram contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 411227,
     "status": "ok",
     "timestamp": 1617582142899,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "qI0jD14NHgod"
   },
   "outputs": [],
   "source": [
    "# A10:Function(5/5)\n",
    "#t-arrow = n-gram\n",
    "\n",
    "def make_context_representations(ngram_frequencies, type_index, CoM):\n",
    "\n",
    "    ngram_semantics = {}\n",
    "    for i in range(len(ngram_frequencies)):\n",
    "        ngram_semantics[i] = Counter()\n",
    "        for ngram in sorted(ngram_frequencies[i]):\n",
    "            \n",
    "            #--- your code starts here\n",
    "            f_t = ngram_frequencies[i][tuple(ngram)]\n",
    "            t = ngram[-1]\n",
    "            t_index = type_index[t]\n",
    "            t_i = CoM[t_index, :]\n",
    "            c = ngram[:-1]\n",
    "            ngram_semantics[i][c] += f_t * np.square(t_i)  \n",
    "            \n",
    "            #--- your code stops here\n",
    "        \n",
    "    return ngram_semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H5xUI7_Hgod"
   },
   "source": [
    "For reference, your output should be:\n",
    "\n",
    "```\n",
    "array([7.97611695e+00, 8.99872340e-03, 2.15038866e-04, 4.26420133e-03,\n",
    "       3.25341871e-04, 1.76035146e-04, 1.74704542e-03, 3.74165424e-06,\n",
    "       5.72603698e-04, 1.23402418e-03, 1.44665778e-03, 1.14847206e-05,\n",
    "       1.53250713e-03, 3.18149604e-04, 1.12255674e-04, 1.78973157e-04,\n",
    "       6.06263814e-04, 6.78922037e-06, 2.12840899e-04, 3.68067133e-04,\n",
    "       7.93624931e-05, 5.41433409e-05, 2.98419701e-05, 1.43090555e-04,\n",
    "       2.20631917e-04, 7.46823189e-05, 6.16067335e-05, 1.85061200e-05,\n",
    "       2.88289832e-05, 1.82055806e-05, 7.61080382e-05, 1.04006281e-05,\n",
    "       9.33963080e-05, 1.32908572e-04, 5.69448595e-06, 3.81600501e-05,\n",
    "       4.63674505e-05, 1.85668763e-05, 9.56517458e-06, 1.80490734e-05,\n",
    "       1.89096497e-05, 2.51367852e-06, 5.16072034e-06, 8.65617541e-05,\n",
    "       5.57011586e-06, 2.83364689e-05, 6.97222406e-05, 1.47325894e-05,\n",
    "       6.56538317e-06, 1.36116788e-04])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 484892,
     "status": "ok",
     "timestamp": 1617582216570,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "6nk3hMQvHgod",
    "outputId": "8a7b8612-2fa3-4a15-adce-330828404b45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.09491358e-01, 2.60286170e-01, 2.34935240e+00, 2.41477748e-01,\n",
       "       3.27120687e+00, 9.57983175e-02, 2.95200542e-03, 8.64430449e-03,\n",
       "       1.64753209e-04, 3.53254269e-04, 2.00267652e-01, 1.74109214e-02,\n",
       "       2.41203616e-02, 4.32334473e-02, 5.93200066e-04, 3.28836994e-02,\n",
       "       6.11567153e-03, 4.97924144e-03, 2.61648300e-02, 5.43861689e-02,\n",
       "       7.74695500e-02, 3.20148982e-02, 1.40884173e-03, 2.21735219e-03,\n",
       "       7.75303322e-03, 2.28725974e-02, 4.35066414e-04, 1.92064604e-02,\n",
       "       1.77977054e-03, 1.75828291e-04, 1.57058489e-03, 1.37739557e-02,\n",
       "       2.75536566e-02, 4.30162065e-02, 9.56687387e-04, 2.76719269e-04,\n",
       "       4.08678114e-02, 1.31895146e-02, 6.61341588e-03, 1.80720664e-02,\n",
       "       4.80854791e-03, 1.29062074e-03, 8.87355810e-03, 1.98261447e-03,\n",
       "       3.47211325e-02, 1.74234468e-02, 1.29041312e-01, 1.35195445e-01,\n",
       "       7.51812252e-02, 1.03757020e-02])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A10:SanityCheck\n",
    "\n",
    "ngram_semantics = make_context_representations(ngram_frequencies, type_index, CoM_d_normed)\n",
    "ngram_semantics[6][tuple(tokenize('robert downey jr.'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjdjWXD1Hgoe"
   },
   "source": [
    "### 11. (7 pts) Generalize the standard LM with CBOW semantic similarity\n",
    "Now that we have an aggregated representation of semantic mass for the types which appear in the $n$-gram contexts (`ngram_semantics`) it's time to build a CBOW-semantic, $n$-gram LM. You job here is to complete the function:\n",
    "```\n",
    "t_Ps = P_next_CBOW(gram, ngram_semantics, CoM, type_index, epsilon = 0.1)\n",
    "```\n",
    "which will operate as an LM in a manner that is quite similar to that of `P_next()`. Here, the new arguments are:\n",
    "- `ngram_semantics`: the $CoM^2_c$ aggregated semantic masses for types in context\n",
    "- `CoM`: a normed co-ocurrence matrix, output from __Part 1__'s `reduced_normed_CoM(newstweet)` function\n",
    "and all other inputs and outpus are the same as for `P_next()`.\n",
    "\n",
    "Here, the LM will once again employ backoff and smoothing. Backoff will be accomplished again by recursively calling the (now `P_next_CBOW()`) function, which will be quite straightforward. However, generalizing the smoothing process will require a bit more work. Here, you'll instead be computing the `t_Ps` object, i.e., $\\hat{P}(t_n|t_1, t_2, \\cdots t_{n-1})$ as a `Counter()` of type-prediction probabilities via the product of independent, semantic-component likelihoods:\n",
    "$$\n",
    "\\hat{P}(t_n|t_1, t_2, \\cdots t_{n-1}) = \n",
    "\\left[\n",
    "\\prod_{k = 1}^d\n",
    "\\frac{CoM_{t_n, k}^2\\left(f(t_1, t_2,\\cdots, t_n) + \\varepsilon\\right) }\n",
    "{CoM_{c,k}^2 + \\varepsilon\\sum_{t\\in W}CoM_{t, k}^2}\n",
    "\\right]^{1/d}\n",
    "$$\n",
    "where here, $k$ ranges over the semantic dimensionality $d$. Note: this formulation is most easily computed via log space as an arithmetic average:\n",
    "$$\n",
    "\\log\\hat{P}(t_n|t_1, t_2, \\cdots t_{n-1}) = \n",
    "\\frac{1}{d}\\sum_{k = 1}^d\n",
    "\\log\\left(\n",
    "\\frac{CoM_{t_n, k}^2\\left(f(t_1, t_2,\\cdots, t_n) + \\varepsilon\\right) }\n",
    "{CoM_{c,k}^2 + \\varepsilon\\sum_{t\\in W}CoM_{t, k}^2}\n",
    "\\right)\n",
    "$$\n",
    "and then exponentiated back to probability space for output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram = tuple(tokenize(\"he goes on an \"))\n",
    "n1 = len(gram)\n",
    "epsilon = 0.1\n",
    "epsilon /= len(type_index)\n",
    "y = []\n",
    "t_Ps = Counter()\n",
    "types = [t for t in type_index]\n",
    "type_indices = [type_index[t] for t in types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.58672955e-03 3.45406756e-03 3.24547592e-02 3.79494287e-03\n",
      " 1.26469245e-01 1.46776957e-02 7.87821789e-05 3.05450437e-05\n",
      " 2.97599549e-08 6.77184200e-09 1.52889512e-02 1.18264114e-02\n",
      " 1.35976404e-02 1.57601465e-02 2.50173858e-03 5.57180879e-03\n",
      " 4.07023786e-03 3.43606754e-03 3.50593044e-02 1.24946242e-01\n",
      " 1.18946755e-01 3.56580610e-02 1.50380511e-03 2.12012485e-03\n",
      " 1.52040428e-05 7.85705017e-02 1.20608039e-02 6.60276062e-03\n",
      " 2.52862476e-04 6.26927464e-05 1.23393963e-02 1.53281393e-03\n",
      " 5.23273498e-03 3.69859909e-02 2.12193086e-03 7.25054839e-02\n",
      " 3.87804461e-03 5.01682383e-03 7.54521505e-02 3.39746900e-02\n",
      " 1.54733793e-04 9.43821504e-03 7.32567890e-03 2.43037895e-03\n",
      " 1.05048474e-02 1.70876621e-02 5.31765250e-03 2.97673538e-05\n",
      " 1.32080793e-03 2.59512732e-02]\n"
     ]
    }
   ],
   "source": [
    "if gram in ngram_semantics[n1]:\n",
    "    print(ngram_semantics[n1][gram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 484892,
     "status": "ok",
     "timestamp": 1617582216572,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "ku_BKxeGHgoe"
   },
   "outputs": [],
   "source": [
    "# A11:Function(7/7)\n",
    "\n",
    "#adapt what you wrote for P_next but for the vectors, instead of using the frequencies, you are using the vectors which you are performing math on\n",
    "\n",
    "def P_next_CBOW(gram, ngram_semantics, CoM, type_index, epsilon = 0.1):\n",
    "    n1 = len(gram)\n",
    "    epsilon /= len(type_index)\n",
    "    y = []\n",
    "    t_Ps = Counter()\n",
    "    types = [t for t in type_index]\n",
    "    type_indices = [type_index[t] for t in types]\n",
    "    if gram in ngram_semantics[n1]: ## use gram to condition frequencies with epsilon-smoothing\n",
    "        \n",
    "        #--- your code starts here\n",
    "        for key in type_index.keys():\n",
    "            y.append((key, ((epsilon + ngram_semantics[n1][gram+(key,)]) / (epsilon*len(type_index) + ngram_semantics[n1-1][gram]))))\n",
    "        \n",
    "        for j in y:\n",
    "            t_Ps[j[0]] = j[1]    \n",
    "\n",
    "        #--- your code stops here\n",
    "        \n",
    "    else: ## recursively back off to lower-n model\n",
    "        \n",
    "        #--- your code starts here\n",
    "        \n",
    "        gram = gram[1:]\n",
    "        print(gram)\n",
    "        return P_next_CBOW(gram, ngram_semantics, type_index, epsilon = .1)\n",
    "\n",
    "        #--- your code stops here\n",
    "    \n",
    "    return t_Ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hB6YoPLLH2E"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "[0.37939464029388503,\n",
    " [('adventure', 0.2818891515803675),\n",
    "  ('andrewcmccarthy', 0.00011029111215864133),\n",
    "  ('lizzzyacker', 0.00011029111215864133),\n",
    "  ('cfbplayoff', 0.00010564393592587125),\n",
    "  ('cowhide', 5.9069417953547754e-05),\n",
    "  ('7b', 5.7778635208333525e-05),\n",
    "  ('selah', 5.5544304078059575e-05),\n",
    "  ('ever-evolving', 5.1995269581865545e-05),\n",
    "  ('incidentally', 5.1995269581865545e-05),\n",
    "  ('realistically', 5.1995269581865545e-05)],\n",
    " [(' ', 0.014840491124005418),\n",
    "  ('andrewcmccarthy', 0.00022725969991163811),\n",
    "  ('lizzzyacker', 0.00022725969991163811),\n",
    "  ('cfbplayoff', 0.00021768398836584537),\n",
    "  ('cowhide', 0.00012171514037114209),\n",
    "  ('7b', 0.00011905542560730372),\n",
    "  ('selah', 0.00011445148779009385),\n",
    "  ('ever-evolving', 0.00010713854571529673),\n",
    "  ('incidentally', 0.00010713854571529673),\n",
    "  ('realistically', 0.00010713854571529673)]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 486361,
     "status": "ok",
     "timestamp": 1617582218048,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "aRqHRv8mHgoe",
    "outputId": "c069eddb-162a-448a-a839-449d9edb11d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " [('\\n', 3.1269543464665416e-05),\n",
       "  (' ', 3.1269543464665416e-05),\n",
       "  ('!', 3.1269543464665416e-05),\n",
       "  ('\"', 3.1269543464665416e-05),\n",
       "  ('#', 3.1269543464665416e-05),\n",
       "  ('$', 3.1269543464665416e-05),\n",
       "  ('%', 3.1269543464665416e-05),\n",
       "  ('&', 3.1269543464665416e-05),\n",
       "  (\"'\", 3.1269543464665416e-05),\n",
       "  (\"''\", 3.1269543464665416e-05)],\n",
       " [('\\n', 3.1269543464665416e-05),\n",
       "  (' ', 3.1269543464665416e-05),\n",
       "  ('!', 3.1269543464665416e-05),\n",
       "  ('\"', 3.1269543464665416e-05),\n",
       "  ('#', 3.1269543464665416e-05),\n",
       "  ('$', 3.1269543464665416e-05),\n",
       "  ('%', 3.1269543464665416e-05),\n",
       "  ('&', 3.1269543464665416e-05),\n",
       "  (\"'\", 3.1269543464665416e-05),\n",
       "  (\"''\", 3.1269543464665416e-05)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A11:SanityCheck\n",
    "\n",
    "[np.nansum([x for x in P_next_CBOW(tuple(tokenize(\"he goes on an \")), ngram_semantics, CoM_d_normed, type_index).values()]), \n",
    " list(P_next_CBOW(tuple(tokenize(\"he goes on an \")), ngram_semantics, CoM_d_normed, type_index).most_common(10)), \n",
    " list(P_next_CBOW(tuple(tokenize(\" he goes on an\")), ngram_semantics, CoM_d_normed, type_index).most_common(10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufHwO1z-Hgof"
   },
   "source": [
    "#### Now let's see what happens when we ramble across the entire vocabulary\n",
    "For reference, your output should be:\n",
    "```\n",
    "generated document, starting from \"robert downey jr\":\n",
    "\n",
    ".'s 'voyage of doctor ned, mad moxxi’s underdome riot, the secret armory of general knoxx, and claptrap’s new robot revolution. \n",
    "\n",
    "average perplexity of ramble:  1.429765446348872\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555395,
     "status": "ok",
     "timestamp": 1617582287088,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "PwkENHoWHgof",
    "outputId": "d200ae94-91a2-4548-c553-2b5037b088c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated document, starting from \"robert downey jr\":\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "P_next() takes from 3 to 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d64d80fe7230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m691\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewstweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m rambling, likelihood = ramble(tuple(document[:5]), 49, \n\u001b[0m\u001b[1;32m      6\u001b[0m                               \u001b[0;34m(\u001b[0m\u001b[0mngram_semantics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCoM_d_normed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                               LM = P_next_CBOW)\n",
      "\u001b[0;32m<ipython-input-17-2a9f911cf7f6>\u001b[0m in \u001b[0;36mramble\u001b[0;34m(prompt, docsize, LM_args, LM, n, top, verbose)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrambling\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdocsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#--- your code starts here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_LM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1gram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLM_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mrambling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1gram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mLM_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-fbfee281bff3>\u001b[0m in \u001b[0;36msample_LM\u001b[0;34m(gram, LM_args, top, LM)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_LM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLM_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mPs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mLM_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mPs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mPs\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mPs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: P_next() takes from 3 to 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "# A11:SanityCheck\n",
    "j = 5\n",
    "np.random.seed(691)\n",
    "document = tokenize(newstweet[j]['text'].lower())\n",
    "rambling, likelihood = ramble(tuple(document[:5]), 49, \n",
    "                              (ngram_semantics, CoM_d_normed, type_index, 0.0001),\n",
    "                              LM = P_next_CBOW)\n",
    "print(\"\\n\\naverage perplexity of ramble: \", perplexity(likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lejgU__tHgog"
   },
   "source": [
    "#### Next, let's see what ramble looks like with document-specific types \n",
    "For reference, your output should be:\n",
    "```\n",
    "generated document, starting from \"robert downey jr\":\n",
    "\n",
    ".'s 'voyage of doctor dolittle' to january 2020\n",
    "\n",
    "“dolittle” tells the story of dr. dolittle in read: “syriana,” “traffic”),\n",
    "\n",
    "average perplexity of ramble:  3.756691687638365\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555678,
     "status": "ok",
     "timestamp": 1617582287377,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "oS7nsY-ZHgog",
    "outputId": "21011e29-a234-41ab-f8cd-663d306c68a6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-82beb9e50574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m691\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewstweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m rambling, likelihood = ramble(tuple(document[:5]), 50, \n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# A11:SanityCheck\n",
    "\n",
    "j = 5\n",
    "np.random.seed(691)\n",
    "document = tokenize(newstweet[j]['text'].lower())\n",
    "rambling, likelihood = ramble(tuple(document[:5]), 50, \n",
    "                              (ngram_semantics, CoM_d_normed, doc_types[j], 0.0001),\n",
    "                              LM = P_next_CBOW)\n",
    "print(\"\\n\\naverage perplexity of ramble: \", perplexity(likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8JYHNnEHgoh"
   },
   "source": [
    "#### Now let's see what the average perplexity of our model is in a recitation over the document types\n",
    "For reference, your output should be:\n",
    "```\n",
    "generated document, starting from \"robert downey \":\n",
    "\n",
    "jr.'s( he have to is.than people as well young trailer. dolittle  but, when well this official “dolittle” trailer released sunday, new released not forced to leave his hideaway and set sail across the sea is a young island.\n",
    "\n",
    "“we have to choice but to embark on this during journey,” downey whispers to his soon and furry friends.\n",
    "\n",
    "and,then’is getting into all sorts of dangerous situations, like being chained in a medieval dungeon with a tiger who greets him with, buthello, lunch” and “ jumps him. dr we for set — a gorilla voiced by rami malek comes to his feathered.\n",
    "\n",
    "also,on: “ moves robert downey jr.'s 'voyage of doctor dolittle' to january 2020 \n",
    "bydolittle” trailer the story along famed doctor and veterinarian during the falls ( queen victoria’s england, dr. dolittle dolittle, who is to action after the team,of his wife on years,earlier,caused him to judge a hermit who only talks to animals. but we we team,people (jessie buckley, “wild rose”) falls ill, he voiced on in adventure to find her a cure. he also in by this story by a young apprentice (harry collett, “dunkirk”) and zachary animal friends, that the anxious gorilla (malek), an upbeat duck (octavia spencer), an cynical ostrich (kumail nanjiani), an enthusiastic polar bear (john cena), and he doctor parrot (emma thompson).zachary\n",
    "read banderas also stars as rassouli, along with his sheen as mudfly. additional voice performers include marion cotillard, frances de la tour, carmen ejogo, ralph fiennes, selena gomez  tom holland  and then robinson.\n",
    "\n",
    "“we” trailer directed by stephen gaghan (“syriana,” “maleficent”), as we by joe roth and jeff kirschenbaum under their roth/kirschenbaum films banner (“alice in wonderland,” “maleficent”)  and well as we susan downey (“sherlock holmes” franchise, “the voice”) for team downey. downey jr.'sexecutive produces along iwth sarah bradshaw (“the mummy,” “maleficent”),and zachary roth (“maleficent: mistress of evil”).\n",
    "\n",
    "average perplexity of recitation:  2.9565105975316377\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 561527,
     "status": "ok",
     "timestamp": 1617582293233,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "94A96DVAHgoh",
    "outputId": "765f6163-86db-49cb-82c8-9bebd72a6ca0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-69aa10d77415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m691\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewstweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m recitation, likelihood = recite(document, (ngram_semantics, CoM_d_normed, doc_types[j], 0.0001),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# A11:SanityCheck\n",
    "\n",
    "j = 5\n",
    "np.random.seed(691)\n",
    "document = newstweet[j]['text'].lower()\n",
    "recitation, likelihood = recite(document, (ngram_semantics, CoM_d_normed, doc_types[j], 0.0001),\n",
    "                                LM = P_next_CBOW)\n",
    "print(\"\\n\\naverage perplexity of recitation: \", perplexity(likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTt7aukCHgoh"
   },
   "source": [
    "#### Finally, let's see what the average perplexity of our model is in a recitation across the entire vocabulary\n",
    "For reference, your output should be:\n",
    "```\n",
    "generated document, starting from \"robert downey \":\n",
    "\n",
    "jr.'sthrived he have to people.than people as well doj fish. shaun  unlike, morbid frenzied this?official toldbake” trailer released sunday, gov doesn't owed forced to militarily his hideaway and set sail across the entryway creature a simple island.\n",
    "\n",
    "“i owe mobs ambitions but to prosecute on quests one journey,” downey whispers to his success and furry friends.\n",
    "\n",
    "and,thank’it getting into debt sorts of schemes situations, 9-on-7 wollin cinderella in a football-centric dungeon with a rear-naked who greets him with, howeverhello, lunch” and michelle reevaluate him. didn if because cheat about a 230 voiced by rami malek comes to see feathered.\n",
    "\n",
    "also,known: delta moves robert downey jr.'s 'voyage of doctor dolittle' to january 2020 \n",
    "floyddolittle” trailer the hollywood is famed doctor and patient during the easter she discharge victoria’s england, 1966. pino's barrasso, who smugly to stewart's after the padres,of c judgement was years,earlier,caused him to anticipate inflamed focus who only had to reuters. but we cops u,family (jessieratifybuckley, “wild rose”) falls ill, gruber said about injured unqualified to fruition deadpool's a&fifth. he played still by stage biological by a conservative-leaning fleet (harry collett, “dunkirk”) and zachary males friends, but mendocino 0-6 gorilla (malek), an appointee duck (octavia spencer), an nih ostrich (kumail nanjiani), an easy polar bear (john rudoff), and nataliya celebration parrot (emma thompson).minions\n",
    "additionally banderas also stars as rassouli, along with household sheen as mudfly. additional reporting performers include marion cotillard, frances de la industria, 1972 ejogo, ralph fiennes, selena gomez  27 jurich  and javier robinson.\n",
    "\n",
    "“oh” trailer turn-based more stephen hawking (“syriana,” “maleficent”), as christabel alongside sony roth and jeff kirschenbaum under their roth/kirschenbaum films banner (“alice in wonderland,” “maleficent”)  and rosengren as jerushah susan downey (“sherlock holmes” franchise, tuathe gateway”) for haskins usa. downey jr.'sol produces along iwth sarah bradshaw (“the mummy,” “maleficent”),and unitas roth (“maleficent: mistress of evil”).\n",
    "\n",
    "average perplexity of recitation:  3.125198389248689\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1600483,
     "status": "ok",
     "timestamp": 1617583332196,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "yinSsqBkHgoh",
    "outputId": "32ceffea-a83a-4b59-e5da-2da22c2b3427"
   },
   "outputs": [],
   "source": [
    "# A11:SanityCheck\n",
    "\n",
    "j = 5\n",
    "np.random.seed(691)\n",
    "document = newstweet[j]['text'].lower()\n",
    "recitation, likelihood = recite(document, (ngram_semantics, CoM_d_normed, type_index, 0.0001),\n",
    "                                LM = P_next_CBOW)\n",
    "print(\"\\n\\naverage perplexity of recitation: \", perplexity(likelihood))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A2-module-A-answer-key.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
