# Natural-language-processing-with-deep-learning-for-data-science
## Extracting tweet-like summaries from the news
### Techniques employed:
- Continuous Bag of Words (CBOW)
- Scikit-learn
- Text and data mining (TDM)
- Inverse document frequency (IDF)
- Logistic regression
- ROUGE-L
- Matplotlib
### Summary: <br/>
News stories can be further condensed into ‘tweet’ like summaries. By operating a statistical
engine, isolating the inverse document frequencies, creating a logistic classifier, and evaluating the
precision, accuracy, and recall metrics of our summarizations. Box-and-whisker plotting was done to
visualize these metrics.

## Project Title: Abstracting summaries of the news
Techniques employed:
- Continuous Bag of Words (CBOW)
- Scikit-learn
- N-gram counting
- Language modeling
- Recitation function
- Perplexity performance evaluation
- Rambling functions
### Summary: <br/> 
This project was performed in order to learn language modeling techniques. By operating a
statistical engine, building n-gram frequencies, building a language model with a model sampler, creating
a recitation and rambling function, and a perplexity performance evaluation function, I was able to
summarize a news story about an upcoming Robert Downey Jr. film.

## Project Title: GloVe semantic representation
Techniques employed:
- Co-occurrence matrix
- Loss functions
- GloVe loss and gradient functions
- Sampling
- Analogy testing
### Summary: <br/> 
This project was performed in order to gain experience with iteration based learning. In this
project, I had to build a model’s training data, weight co-occurrence matrices, implement a loss function,
build a sampling function, operate the GloVe function, and evaluate the performance of the model.

## Project Title: Conversational disentanglement
Techniques employed:
- Tokenization
- PyTorch
- Loss function
- Train function
- Evaluation function
- Position embedding
- Time embedding
### Summary: <br/> 
Conversational disentanglement allows for individuals to know which speakers express which
sentiments or attitudes. This project required the use of tokenization, constructing a PyTorch dataset,
constructing a network architecture, constructing a loss function, building an optimizer, creating an
evaluation function, implementing position embedding, and a time embedding. In addition to this, a linear
layer was used to evaluate the success of the network.

## Project Title: Open Information Extraction (OIE)
Techniques employed:
- Recurrent neural networks (RNN)
- Tokenization
- Part of speech tagging
- Multi modal vector representation
- Neural LSTM tagging
### Summary: <br/> 
Recurrent neural networks can be used for tagging tasks. In this exercise, I created a
recurrent neural network that performed part-of-speech (POS) tagging. The model was evaluated using
scikit-learn’s classification report.

## Project Title: Abstractive Summarization of Scientific Papers with BART
Techniques employed:
- BART
- Hugging Face
- Text summarization
- ROUGE metrics
### Summary: <br/> 
Scientific texts are summarized in an abstract. In this project, my team and I fine-tuned a
pre-trained BART model to perform an abstractive summarization of a scientific document in order to
generate an abstract. In addition, this allowed us to gain further experience and understanding of using
Hugging Face.
